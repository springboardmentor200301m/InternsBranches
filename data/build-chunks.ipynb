{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91950a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, json, unicodedata\n",
    "from collections import defaultdict, OrderedDict\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b39d39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FOLDER = \"files\"\n",
    "MAPPING_PATH = \"src/role_mapping.json\"   \n",
    "OUT_CHUNKS = \"chunks.json\"\n",
    "OUT_ROLE_MAP = \"role_to_chunks.json\"\n",
    "CHUNK_SIZE = 512\n",
    "CHUNK_OVERLAP = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81b2c42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_raw(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def read_csv(path):\n",
    "    df = pd.read_csv(path, dtype=str).fillna(\"\")\n",
    "    parts = []\n",
    "    header = \" | \".join(df.columns.tolist())\n",
    "    parts.append(header)\n",
    "    for _, row in df.iterrows():\n",
    "        parts.append(\" | \".join(row.astype(str).tolist()))\n",
    "    return \"\\n\".join(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d49c92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADING_RE = re.compile(r'^(#{1,6})\\s*(.+)$', flags=re.MULTILINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cfbd469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_title_and_sections(md_text, filename):\n",
    "    text = md_text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    matches = list(HEADING_RE.finditer(text))\n",
    "    if not matches:\n",
    "        title = os.path.splitext(os.path.basename(filename))[0]\n",
    "        return title, [{\"heading\": None, \"content\": text.strip()}]\n",
    "    title = None\n",
    "    if matches:\n",
    "        for m in matches:\n",
    "            if len(m.group(1)) == 1:   \n",
    "                title = m.group(2).strip()\n",
    "                break\n",
    "    if not title:\n",
    "        title = os.path.splitext(os.path.basename(filename))[0]\n",
    "    sections = []\n",
    "    for i, m in enumerate(matches):\n",
    "        heading_text = m.group(2).strip()\n",
    "        start = m.end()\n",
    "        end = matches[i+1].start() if i+1 < len(matches) else len(text)\n",
    "        body = text[start:end].strip()\n",
    "        sections.append({\"heading\": heading_text, \"content\": body})\n",
    "    return title, sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "480196a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    text = re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f]', ' ', text)\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "    text = re.sub(r'[ \\t]{2,}', ' ', text)\n",
    "    text = \"\\n\".join(line.strip() for line in text.splitlines())\n",
    "    text = re.sub(r' {2,}', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "730a0ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    if not text:\n",
    "        return []\n",
    "    return text.split()\n",
    "\n",
    "def detokenize(tokens):\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fe751b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_tokens(tokens, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):\n",
    "    if not tokens:\n",
    "        return []\n",
    "    chunks = []\n",
    "    step = chunk_size - overlap if (chunk_size - overlap) > 0 else chunk_size\n",
    "    for start in range(0, len(tokens), step):\n",
    "        end = start + chunk_size\n",
    "        chunk = tokens[start:end]\n",
    "        if not chunk:\n",
    "            break\n",
    "        chunks.append(chunk)\n",
    "        if end >= len(tokens):\n",
    "            break\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d390eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_role_map(path=MAPPING_PATH):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        tag_map = json.load(f, object_pairs_hook=OrderedDict)\n",
    "    norm = OrderedDict()\n",
    "    for role, kws in tag_map.items():\n",
    "        norm[role] = [k.lower() for k in kws if isinstance(k, str)]\n",
    "    return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b3ee425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roles_for_folder(folder_name, role_map):\n",
    "    folder = (folder_name or \"\").lower()\n",
    "    roles = []\n",
    "    for role, kws in role_map.items():\n",
    "        for k in kws:\n",
    "            if k == folder:\n",
    "                roles.append(role)\n",
    "                break\n",
    "    if \"C-Level\" in role_map:\n",
    "        roles.append(\"C-Level\")\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for r in roles:\n",
    "        if r not in seen:\n",
    "            out.append(r); seen.add(r)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c80a15ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_chunks(input_folder=INPUT_FOLDER, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):\n",
    "    role_map = load_role_map()\n",
    "    all_chunks = []\n",
    "    role_to_chunkids = defaultdict(list)\n",
    "\n",
    "    doc_id = 0\n",
    "    for root, _, files in os.walk(input_folder):\n",
    "        for fname in sorted(files):\n",
    "            path = os.path.join(root, fname)\n",
    "            ext = os.path.splitext(fname)[1].lower()\n",
    "            try:\n",
    "                if ext in [\".md\", \".markdown\"]:\n",
    "                    raw = read_file_raw(path)\n",
    "                    title, sections = extract_title_and_sections(raw, fname)\n",
    "                elif ext == \".csv\":\n",
    "                    raw = read_csv(path)\n",
    "                    title = os.path.splitext(fname)[0]\n",
    "                    sections = [{\"heading\": None, \"content\": raw}]\n",
    "                elif ext in [\".txt\"]:\n",
    "                    raw = read_file_raw(path)\n",
    "                    title = os.path.splitext(fname)[0]\n",
    "                    sections = [{\"heading\": None, \"content\": raw}]\n",
    "                else:\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                print(f\"[warning] failed to read {path}: {e}\")\n",
    "                continue\n",
    "\n",
    "            department = os.path.basename(root).lower()\n",
    "            accessible_roles = roles_for_folder(department, role_map)\n",
    "\n",
    "            for s_idx, sec in enumerate(sections):\n",
    "                sec_heading = sec.get(\"heading\")\n",
    "                sec_text = sec.get(\"content\", \"\")\n",
    "                combined_text = \" \".join([title or \"\", sec_heading or \"\", sec_text or \"\"]).strip()\n",
    "                cleaned = clean_text(combined_text)\n",
    "                tokens = tokenize(cleaned)\n",
    "                token_chunks = chunk_tokens(tokens, chunk_size=chunk_size, overlap=overlap)\n",
    "\n",
    "                for c_idx, tk in enumerate(token_chunks):\n",
    "                    chunk_id = f\"doc{doc_id}_sec{s_idx}_chunk{c_idx}\"\n",
    "                    chunk_text = detokenize(tk)\n",
    "                    meta = {\n",
    "                        \"chunk_id\": chunk_id,\n",
    "                        \"source\": path,\n",
    "                        \"filename\": fname,\n",
    "                        \"doc_id\": doc_id,\n",
    "                        \"section_index\": s_idx,\n",
    "                        \"section_heading\": sec_heading,\n",
    "                        \"title\": title,\n",
    "                        \"department\": department,\n",
    "                        \"roles\": accessible_roles,\n",
    "                        \"token_count\": len(tk)\n",
    "                    }\n",
    "                    all_chunks.append({\"id\": chunk_id, \"text\": chunk_text, \"meta\": meta})\n",
    "                    for r in accessible_roles:\n",
    "                        role_to_chunkids[r].append(chunk_id)\n",
    "\n",
    "            doc_id += 1\n",
    "\n",
    "    return all_chunks, dict(role_to_chunkids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7059c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    chunks, role_map_out = build_chunks()\n",
    "    with open(OUT_CHUNKS, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(chunks, f, ensure_ascii=False, indent=2)\n",
    "    with open(OUT_ROLE_MAP, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(role_map_out, f, ensure_ascii=False, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
